{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3baaadd3",
   "metadata": {},
   "source": [
    "<h1>Table of Contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#import_libraries\">Import Libraries</a></li>\n",
    "        <li><a href=\"#import_dataset\">Import \"Pima Indians Diabetes\" Dataset</a></li>\n",
    "        <li><a href=\"#information\">Information about the Dataset</a></li>\n",
    "        <li><a href=\"#pre-processing\">Pre-processing</a></li>\n",
    "        <li><a href=\"#final_dataset\">Final dataset after Pre-processing</a></li>\n",
    "        <li><a href=\"#feature_selection\">Feature Selection</a></li>\n",
    "        <li><a href=\"#classification\">Classification</a></li>        \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a881b",
   "metadata": {},
   "source": [
    "<div id=\"import_libraries\"> \n",
    "    <h2>Import Libraries</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a77c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.feature_selection import RFE \n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009856f2",
   "metadata": {},
   "source": [
    "<div id=\"import_dataset\"> \n",
    "    <h2>Import \"Pima Indians Diabetes\" Dataset</h2>         \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52138e4",
   "metadata": {},
   "source": [
    "**About the dataset :**\n",
    "<ul>\n",
    "       <li> The \"Pima Indians Diabetes\" dataset includes medical data for 768 women of Pima Indian descent, aimed at predicting diabetes onset. It consists of features such as the number of pregnancies, glucose levels, blood pressure, skin thickness, insulin levels, body mass index (BMI), diabetes pedigree function, and age. The target variable indicates whether an individual has diabetes (1) or not (0). \n",
    "        <br>\n",
    "        <br>\n",
    "        <li> This dataset is commonly used in machine learning for developing predictive models and understanding diabetes risk factors.\n",
    "        <br>\n",
    "        <br>\n",
    "        <li> By analyzing this dataset, researchers can identify significant predictors of diabetes, enhancing early detection and informing healthcare strategies for prevention and intervention in at-risk populations. \n",
    "</ul> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b70cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"  \n",
    "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',  \n",
    "                'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']  \n",
    "pid_df = pd.read_csv(url, header=None, names=column_names)\n",
    "display(pid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99363ebb",
   "metadata": {},
   "source": [
    "<div id=\"information\"> \n",
    "    <h2>Information about the Dataset</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics for the dataset\n",
    "# This includes count, mean, standard deviation, minimum, 25%, 50%, 75%, and maximum values for numeric columns\n",
    "print('\\nThe dataset description:\\n')\n",
    "\n",
    "data_describe = pid_df.describe()\n",
    "display(data_describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b742610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a concise summary of the dataset\n",
    "# This summary includes the index dtype, column dtypes, non-null values, and memory usage \n",
    "print('\\nMore information about the dataset:\\n')\n",
    "\n",
    "data_information = pid_df.info()\n",
    "display(data_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the dataset, which returns the number of rows and columns\n",
    "shape_of_the_dataset = pid_df.shape\n",
    "print(\"\\nThe shape of the dataset -->\", shape_of_the_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique values in each column of the dataset\n",
    "print('\\nNumber of unique data in the dataset:\\n')\n",
    "\n",
    "unique_data = pid_df.nunique()\n",
    "print(unique_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb4ec6",
   "metadata": {},
   "source": [
    "<div id=\"pre-processing\"> \n",
    "    <h2>Pre-processing</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#duplicates\">Duplicate Tuples</a></li>\n",
    "        <li><a href=\"#outliers\">Detecting Outliers (Noise)</a></li>\n",
    "        <li><a href=\"#missing_values\">Handling Missing Values</a></li>\n",
    "        <li><a href=\"#standardization\">Standardization</a></li>\n",
    "        <li><a href=\"#data_imbalance\">Handling Data Imbalance</a></li>      \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c7625",
   "metadata": {},
   "source": [
    "<div id=\"duplicates\"> \n",
    "    <h2>Duplicate Tuples</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74119aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of duplicate rows in the dataframe\n",
    "Num_of_duplicate_rows = pid_df.duplicated().sum()\n",
    "print(\"\\nThe number of duplicate rows -->\", Num_of_duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af837725",
   "metadata": {},
   "source": [
    "<div id=\"outliers\"> \n",
    "    <h2>Detecting Outliers (Noise)</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#iqr\">Interquartile Range (IQR) method</a></li>          \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e56794d",
   "metadata": {},
   "source": [
    "<div id=\"iqr\"> \n",
    "    <h2>Interquartile Range (IQR) method</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4eac1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)  \n",
    "Q1 = pid_df.quantile(0.25)  \n",
    "Q3 = pid_df.quantile(0.75)  \n",
    "IQR = Q3 - Q1  \n",
    "\n",
    "# Define the outlier detection bounds  \n",
    "lower_bound = Q1 - 1.5 * IQR  \n",
    "upper_bound = Q3 + 1.5 * IQR  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d148754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to filter out rows with outliers  \n",
    "outlier_mask = ~((pid_df < lower_bound) |   \n",
    "                 (pid_df > upper_bound)).any(axis=1)  \n",
    "\n",
    "# Create a new dataframe after outlier detection and deleting\n",
    "df_iqr = pid_df[outlier_mask]  \n",
    "display(df_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e09583ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the IQR method\n",
    "# Separate features and target variable  \n",
    "x = df_iqr.drop('Outcome', axis=1)            # Features\n",
    "y = df_iqr['Outcome']                         # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f11a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier  \n",
    "clf_iqr = KNeighborsClassifier(n_neighbors=1)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after IQR outlier removal  \n",
    "accuracy = np.mean(cross_val_score(clf_iqr, x_train, y_train, scoring='accuracy', cv=10))  \n",
    "print(f'\\nCross-validated accuracy after IQR outlier removal: {accuracy:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ad597",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset shape before deleting the outliers -->\", pid_df.shape)\n",
    "print(\"\\nDataset shape after deleting the outliers -->\", df_iqr.shape)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b69b58",
   "metadata": {},
   "source": [
    "<div id=\"missing_values\"> \n",
    "    <h2>Handling Missing Values</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#mean\">Mean Imputation</a></li>\n",
    "        <li><a href=\"#iterative\">Iterative Imputation</a></li>\n",
    "        <li><a href=\"#knn\">K-Nearest Neighbors (KNN) Imputation</a></li>\n",
    "        <li><a href=\"#output\">Output the results</a></li>    \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics after detecting outliers\n",
    "# This includes count, mean, standard deviation, minimum, 25%, 50%, 75%, and maximum values for numeric columns\n",
    "print('\\nThe data set description after detecting outliers:\\n')\n",
    "display(df_iqr.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cad71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataframe\n",
    "isna = pd.DataFrame(df_iqr.isna().sum(axis=0))\n",
    "print(isna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b1d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nThere are no NaN values in the dataset \\n')\n",
    "print('\\nBut according to the description, some variables cannot be zero. So they must be handled')\n",
    "print('They are --> SkinThickness and Insulin \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf4e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zero values with NaN in specified columns \n",
    "temp= ['SkinThickness', 'Insulin']\n",
    "df_iqr[temp]= df_iqr[temp].replace(0, np.nan)\n",
    "\n",
    "# Check for missing values after replacement\n",
    "isna= pd.DataFrame(df_iqr.isna().sum(axis=0))\n",
    "print(isna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812d905",
   "metadata": {},
   "source": [
    "<div id=\"mean\"> \n",
    "    <h2>Mean Imputation</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92199bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataframe for mean imputation  \n",
    "DfMean = df_iqr.copy(deep=True)  \n",
    "\n",
    "# Initialize the simpleimputer for mean imputation  \n",
    "MeanImputer = SimpleImputer(missing_values=np.nan, strategy='mean')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48abc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mean imputation   \n",
    "DfMean.iloc[:, :] = MeanImputer.fit_transform(DfMean)  \n",
    "\n",
    "# Preview the data after mean imputation  \n",
    "print('\\nPreview the data after mean imputation: \\n')\n",
    "display(DfMean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f26101",
   "metadata": {},
   "source": [
    "<div id=\"iterative\"> \n",
    "    <h2>Iterative Imputation</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64f86111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset  \n",
    "DfIterative = df_iqr.copy(deep=True)  \n",
    "\n",
    "# Set up the iterative imputer\n",
    "imputer_ite = IterativeImputer(missing_values=np.nan, sample_posterior=True, min_value=0,\n",
    "                                            random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b53434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfore the imputation\n",
    "DfIterative.iloc[:, :] = imputer_ite.fit_transform(DfIterative)\n",
    "\n",
    "# Preview the data after iterative imputation\n",
    "print('\\nPreview the data after iterative imputation: \\n')\n",
    "display(DfIterative.head())               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97af934",
   "metadata": {},
   "source": [
    "<div id=\"knn\"> \n",
    "\t\t<h2>K-Nearest Neighbors (KNN) Imputation</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset  \n",
    "Df_knn = df_iqr.copy() \n",
    "\n",
    "# Initialize the KNN imputer  \n",
    "imputer_knn = KNNImputer(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162745fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the imputer and transform the dataset  \n",
    "imputed_data_knn = imputer_knn.fit_transform(Df_knn)\n",
    "\n",
    "# Convert back to dataframe  \n",
    "Df_imputed_knn = pd.DataFrame(imputed_data_knn, columns=Df_knn.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data after KNN imputation\n",
    "print('\\nPreview the data after KNN imputation: \\n')\n",
    "display(Df_imputed_knn.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc6d64",
   "metadata": {},
   "source": [
    "<div id=\"output\"> \n",
    "    <h2>Output the results</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7727a30",
   "metadata": {},
   "source": [
    "Compare the different Imputation Methods using **Kernel Density Estimation (KDE) Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a71a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'SkinThickness' column\n",
    "# Setup the plotting environment  \n",
    "plt.figure(figsize=(14, 10))  \n",
    "\n",
    "# KDE for 'SkinThickness' column  \n",
    "sns.kdeplot(df_iqr['SkinThickness'], label='Baseline', fill=False, bw_adjust=0.5)  \n",
    "sns.kdeplot(DfMean['SkinThickness'], label='Mean Imputation', fill=False, bw_adjust=0.5)  \n",
    "sns.kdeplot(DfIterative['SkinThickness'], label='Iterative Imputation', fill=False, bw_adjust=0.5)\n",
    "sns.kdeplot(Df_imputed_knn['SkinThickness'], label='KNN Imputation', fill=False, bw_adjust=0.5) \n",
    "\n",
    "# Aesthetic aspects of the plot  \n",
    "plt.title('KDE Plot comparison of SkinThickness across Imputation Methods')  \n",
    "plt.xlabel('SkinThickness')  \n",
    "plt.ylabel('Density')  \n",
    "plt.legend()  \n",
    "plt.grid(True)  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Insulin' column\n",
    "# Setup the plotting environment  \n",
    "plt.figure(figsize=(14, 10))  \n",
    "\n",
    "# KDE for 'Insulin' column\n",
    "sns.kdeplot(df_iqr['Insulin'], label='Baseline', fill=False, bw_adjust=0.5)  \n",
    "sns.kdeplot(DfMean['Insulin'], label='Mean Imputation', fill=False, bw_adjust=0.5)     \n",
    "sns.kdeplot(DfIterative['Insulin'], label='Iterative Imputation', fill=False, bw_adjust=0.5)\n",
    "sns.kdeplot(Df_imputed_knn['Insulin'], label='KNN Imputation', fill=False, bw_adjust=0.5) \n",
    "\n",
    "# Aesthetic aspects of the plot\n",
    "plt.title('KDE Plot comparison of Insulin across Imputation Methods')  \n",
    "plt.xlabel('Insulin')  \n",
    "plt.ylabel('Density')  \n",
    "plt.legend()  \n",
    "plt.grid(True)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22788914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nContinue working with iterative imputation after comparing different imputation methods:\\n\")\n",
    "display(DfIterative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24e39f",
   "metadata": {},
   "source": [
    "<div id=\"standardization\"> \n",
    "    <h2>Standardization</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#z-score\">Z-Score Standardization (Standard Scaling)</a></li>\n",
    "        <li><a href=\"#min-max\">Min-Max Scaling (Normalization)</a></li> \n",
    "        <li><a href=\"#output\">Output the results</a></li>     \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0cf61",
   "metadata": {},
   "source": [
    "<div id=\"z-score\"> \n",
    "    <h2>Z-Score Standardization (Standard Scaling)</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0349f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Z-score standardization\n",
    "Z_scaler = StandardScaler()  \n",
    "Z_Scaled = Z_scaler.fit_transform(DfIterative)\n",
    "\n",
    "# Create a new dataframe with the scaled data  \n",
    "df_Z_Scaled = pd.DataFrame(Z_Scaled, columns = list(DfIterative.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8511541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all columns except 'Outcome'\n",
    "df_Z_Scaled_final = df_Z_Scaled.drop('Outcome', axis = 1)\n",
    "\n",
    "# Add the 'Outcome' column back to the dataframe\n",
    "df_Z_Scaled_final['Outcome'] = DfIterative['Outcome'].tolist()\n",
    "display(df_Z_Scaled_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "073339f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Z-score standardization\n",
    "# Separate features and target variable  \n",
    "x_z = df_Z_Scaled_final.drop('Outcome', axis = 1)               # Features\n",
    "y_z = df_Z_Scaled_final['Outcome']                              # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20)  \n",
    "x_train_z, x_test_z, y_train_z, y_test_z = train_test_split(x_z, y_z, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier  \n",
    "clf_z = KNeighborsClassifier(n_neighbors=10)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after the Z-standard scaling  \n",
    "accuracy_z = np.mean(cross_val_score(clf_z, x_train_z, y_train_z, scoring='accuracy', cv=10)) \n",
    "print(f'\\nCross-validated accuracy after the Z-standard scaling: {accuracy_z:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6a7a5",
   "metadata": {},
   "source": [
    "<div id=\"min-max\"> \n",
    "    <h2>Min-Max Scaling (Normalization)</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92162a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Min Max scaler\n",
    "MM_scaler = MinMaxScaler()\n",
    "Min_Max_Scaled = MM_scaler.fit_transform(DfIterative)\n",
    "\n",
    "# Create a new dataframe with the scaled data \n",
    "df_Min_Max_Scaled_final = pd.DataFrame(Min_Max_Scaled, columns = list(DfIterative.columns))\n",
    "display(df_Min_Max_Scaled_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3be8c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Min Max Scaler\n",
    "# Separate features and target variable  \n",
    "x_mm = df_Min_Max_Scaled_final.drop('Outcome', axis = 1)            # Features\n",
    "y_mm = df_Min_Max_Scaled_final['Outcome']                           # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20) \n",
    "x_train_mm, x_test_mm, y_train_mm, y_test_mm = train_test_split(x_mm, y_mm, test_size=0.2, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier  \n",
    "clf_mm = KNeighborsClassifier(n_neighbors=10)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after the Min Max scaling  \n",
    "accuracy_mm = np.mean(cross_val_score(clf_mm, x_train_mm, y_train_mm, scoring='accuracy', cv=10))\n",
    "print(f'\\nCross-validated accuracy after the Min Max scaling: {accuracy_mm:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77a9a6",
   "metadata": {},
   "source": [
    "<div id=\"output\"> \n",
    "    <h2>Output the results</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07462c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results of different validation methods   \n",
    "print('\\nZ-standard scaling result:', accuracy_z)  # Print accuracy score for the z-score standardization method  \n",
    "print('\\nMin Max scaling result:', accuracy_mm)    # Print accuracy score for the min max scaler method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nContinue working with the dataset scaled by the Min Max Scaler after comparing different scaling methods:\\n\")\n",
    "df_Scaled = df_Min_Max_Scaled_final\n",
    "display(df_Scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca93550",
   "metadata": {},
   "source": [
    "<div id=\"data_imbalance\"> \n",
    "    <h2>Handling Data Imbalance</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#oversampling\">Oversampling</a></li>\n",
    "        <li><a href=\"#undersampling\">Undersampling</a></li> \n",
    "        <li><a href=\"#output\">Output the results</a></li>   \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369eab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the 'Outcome' variable\n",
    "outcome_counts = df_Scaled['Outcome'].value_counts()  \n",
    "print(\"Outcome distribution:\\n\", outcome_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2626ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable \n",
    "X = df_Scaled.drop('Outcome', axis = 1)             # Features\n",
    "y = df_Scaled['Outcome']                            # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39d28d",
   "metadata": {},
   "source": [
    "<div id=\"oversampling\"> \n",
    "    <h2>Oversampling</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0278541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SMOTE \n",
    "sm = SMOTE()\n",
    "\n",
    "# Print class distribution before oversampling\n",
    "print(\"\\nClass 1 before oversampling --> \", sum(Y_train == 1))\n",
    "print(\"\\nClass 0 before oversampling --> \", sum(Y_train == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e92e0b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the training data\n",
    "# X_train after oversampling --> X_train_OS\n",
    "# Y_train after oversampling --> Y_train_OS\n",
    "X_train_OS, Y_train_OS = sm.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the datasets after oversampling\n",
    "print(\"\\nThe shape of X after oversampling -->\", X_train_OS.shape)\n",
    "print(\"\\nThe shape of Y after oversampling -->\", Y_train_OS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735383de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class distribution after oversampling \n",
    "print(\"\\nClass 1 after oversampling --> \", sum(Y_train_OS == 1))\n",
    "print(\"\\nClass 0 after oversampling --> \", sum(Y_train_OS == 0))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate oversampling\n",
    "# Initialize the KNN classifier  \n",
    "clf_os = KNeighborsClassifier(n_neighbors=1)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after oversampling  \n",
    "accuracy_os = np.mean(cross_val_score(clf_os, X_train_OS, Y_train_OS, scoring='accuracy', cv=10))\n",
    "print(f'\\nCross-validated accuracy after oversampling: {accuracy_os:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d235f3",
   "metadata": {},
   "source": [
    "<div id=\"undersampling\"> \n",
    "    <h2>Undersampling</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96036ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NearMiss  \n",
    "nr = NearMiss()  \n",
    "\n",
    "# Print class distribution before undersampling  \n",
    "print(\"\\nClass 1 before undersampling --> \", sum(Y_train == 1))  \n",
    "print(\"\\nClass 0 before undersampling --> \", sum(Y_train == 0))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "503102a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NearMiss to the training data  \n",
    "# X_train after undersampling --> X_train_US\n",
    "# Y_train after undersampling --> Y_train_US\n",
    "X_train_US, Y_train_US = nr.fit_resample(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2957d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the datasets after undersampling  \n",
    "print(\"\\nThe shape of X after undersampling -->\", X_train_US.shape)  \n",
    "print(\"\\nThe shape of Y after undersampling -->\", Y_train_US.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a374e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class distribution after undersampling  \n",
    "print(\"\\nClass 1 after undersampling --> \", sum(Y_train_US == 1))  \n",
    "print(\"\\nClass 0 after undersampling --> \", sum(Y_train_US == 0))  \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9847d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate undersampling\n",
    "# Initialize the KNN classifier  \n",
    "clf_us = KNeighborsClassifier(n_neighbors=1)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after undersampling  \n",
    "accuracy_us = np.mean(cross_val_score(clf_us, X_train_US, Y_train_US, scoring='accuracy', cv=10))\n",
    "print(f'\\nCross-validated zacuracy after undersampling: {accuracy_us:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aecbff",
   "metadata": {},
   "source": [
    "<div id=\"output\"> \n",
    "    <h2>Output the results</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results of different handling imbalanced data methods   \n",
    "print('\\nOversampling result:', accuracy_os)     # Print accuracy score for oversampling method  \n",
    "print('\\nUndersampling result:', accuracy_us)    # Print accuracy score for undersampling method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81955d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nContinue working with the dataset handled by oversampling after comparing different methods\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85f6d7",
   "metadata": {},
   "source": [
    "<div id=\"final_dataset\"> \n",
    "    <h2>Final dataset after Pre-processing</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "X = pd.DataFrame(X_train_OS)\n",
    "Y = pd.DataFrame(Y_train_OS, columns = ['Outcome'])\n",
    "\n",
    "# Combine features and target for the training set\n",
    "df_train_final = pd.concat([X, Y], axis = 'columns')\n",
    "display(df_train_final.head())              # Preview the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing set\n",
    "# Combine features and target for the testing set\n",
    "df_test_final = pd.concat([X_test, Y_test], axis = 'columns')\n",
    "display(df_test_final.head())              # Preview the testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee75859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final training and test datasets\n",
    "print('\\nThe shape of training dataset -->', df_train_final.shape)\n",
    "print('\\nThe shape of testing dataset -->', df_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training and testing set together\n",
    "# df_final_adp --> df_final_after data preprocessing\n",
    "df_final_adp = pd.concat([df_train_final,df_test_final])      \n",
    "display(df_final_adp.head())                 # Preview the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nThe shape of the combined dataset -->', df_final_adp.shape)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a060711",
   "metadata": {},
   "source": [
    "<div id=\"feature_selection\"> \n",
    "    <h2>Feature Selection</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#fm\">Filter Method (Correlation Analysis)</a></li>\n",
    "        <li><a href=\"#rfe\">Recursive Feature Elimination (RFE)</a></li>         \n",
    "        <li><a href=\"#output\">Output the results</a></li> \t\t\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa64d316",
   "metadata": {},
   "source": [
    "<div id=\"fm\"> \n",
    "    <h2>Filter Method (Correlation Analysis)</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45189ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr = df_final_adp.corr()\n",
    "print('\\nCorrelation between the features in the dataset:\\n')\n",
    "\n",
    "# Display the correlation matrix\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  \n",
    "    display(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ccf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation matrix using a heatmap plot\n",
    "# Setup the plotting environment \n",
    "plt.figure(figsize=(15,15))\n",
    "print('\\nVisualizing the correlation of the dataset:\\n')\n",
    "\n",
    "# Heatmap plot for correlation\n",
    "sns.heatmap(corr, cbar=True, square= True, fmt='.2f', annot=True, annot_kws={'size':10}, cmap='Blues')\n",
    "\n",
    "# Aesthetic aspects of the plot\n",
    "plt.title('Feature Correlation Heatmap', fontsize=18)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fff891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and print correlation of 'Outcome' with other features\n",
    "print(\"\\nThe correlation of 'Outcome' with other features:\\n\")\n",
    "outcome_corr = df_final_adp.corr()['Outcome'].sort_values(ascending=False)  \n",
    "print(outcome_corr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features with correlation >= 0.2 with 'Outcome'\n",
    "significant_features_fm = outcome_corr[outcome_corr >= 0.2].index.tolist()  \n",
    "\n",
    "# Remove 'Outcome' from the list of significant features\n",
    "significant_features_fm = [feature for feature in significant_features_fm if feature != 'Outcome']           \n",
    "print(\"\\nChoosing features that have correlation >= 0.2':\\n\", significant_features_fm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ccb666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate filter method\n",
    "# Separate features and target variable\n",
    "x_fm = df_final_adp[significant_features_fm].drop(columns=['Outcome'], errors='ignore')               # Features \n",
    "y_fm = df_final_adp['Outcome']                                                                        # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier\n",
    "clf_fm = KNeighborsClassifier(n_neighbors=1)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after filter method \n",
    "accuracy_fm = np.mean(cross_val_score(clf_fm, x_fm, y_fm, scoring='accuracy', cv=10))  \n",
    "print(f\"\\nCross-validated accuracy after filter method: {accuracy_fm:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc71a4",
   "metadata": {},
   "source": [
    "<div id=\"rfe\"> \n",
    "    <h2>Recursive Feature Elimination (RFE)</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e79da9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable  \n",
    "X = df_final_adp.drop('Outcome', axis=1)           # Features\n",
    "y = df_final_adp['Outcome']                        # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model  \n",
    "reg_model = LogisticRegression(max_iter=1000)                   # Added max_iter for convergence if needed  \n",
    "\n",
    "# Initialize and fit RFE  \n",
    "rfe = RFE(estimator=reg_model, n_features_to_select=6)          # Select 6 features  \n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41386f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected features  \n",
    "significant_features_rfe = X.columns[rfe.support_]  \n",
    "print(\"Selected features using RFE:\")  \n",
    "print(significant_features_rfe.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61156820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate RFE\n",
    "# Separate features and target variable\n",
    "X_rfe = df_final_adp[significant_features_rfe]           # Features\n",
    "y = df_final_adp['Outcome']                              # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier\n",
    "clf_rfe = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "# Perform cross-validation to check accuracy after RFE\n",
    "accuracy_rfe = np.mean(cross_val_score(clf_rfe, X_rfe, y, scoring='accuracy', cv=10))\n",
    "print(f\"\\nCross-validated accuracy after RFE: {accuracy_rfe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634ca74",
   "metadata": {},
   "source": [
    "<div id=\"output\"> \n",
    "    <h2>Output the results</h2>    \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6af3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results of different feature selection methods   \n",
    "print('\\nFilter method result:', accuracy_fm)     # Print accuracy score for filter (correlation analysis) method  \n",
    "print('\\nRFE result:', accuracy_rfe)              # Print accuracy score for recursive feature elimination (RFE) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d85422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nUsing features that obtained from Recursive Feature Elimination (RFE) because it had better accuracy \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a67867",
   "metadata": {},
   "source": [
    "**Final dataset** after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d49630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset after feature selection (Recursive Feature Elimination (RFE))\n",
    "# Extract the names of the selected features \n",
    "print('\\nselected features:\\n', significant_features_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final dataset with the selected features and add the target column\n",
    "df_final = df_final_adp[significant_features_rfe] \n",
    "df_final['Outcome'] = df_final_adp['Outcome']\n",
    "\n",
    "# Display the final dataset\n",
    "print(\"Final dataset with selected features and target column:\") \n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ecce2",
   "metadata": {},
   "source": [
    "<div id=\"classification\"> \n",
    "    <h2>Classification</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#nb\">Naive Bayes</a></li>   \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2734558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable  \n",
    "X = df_final.drop('Outcome', axis=1)              # Features  \n",
    "y = df_final['Outcome']                           # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20) \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1eb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nThe shape of the X_train dataset -->', X_train.shape)\n",
    "print('\\nThe shape of the Y_train dataset -->', Y_train.shape)\n",
    "print('\\nThe shape of the X_test dataset -->', X_test.shape)\n",
    "print('\\nThe shape of the Y_test dataset -->', Y_test.shape)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0475fc0",
   "metadata": {},
   "source": [
    "<div id=\"nb\">   \n",
    "    <h2>Naive Bayes</h2>    \n",
    "</div>  \n",
    "<div>  \n",
    "    <ol>  \n",
    "        <li>  \n",
    "            <a href=\"#valid\">Validating</a>  \n",
    "            <ol>   \n",
    "                <li><a href=\"#holdout\">Holdout</a></li>   \n",
    "                <li><a href=\"#rrs\">Repeated Random Sampling</a></li>    \n",
    "            </ol>  \n",
    "        </li>  \n",
    "        <li><a href=\"#test\">Testing</a></li>  \n",
    "        <li><a href=\"#roc\">ROC plot and AUC score</a></li> \n",
    "        <li><a href=\"#output\">Output the results</a></li> \n",
    "    </ol>  \n",
    "</div>  \n",
    "<br>  \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb1e41",
   "metadata": {},
   "source": [
    "<div id=\"holdout\"> \n",
    "    <h2>Holdout</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd40b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout \n",
    "# Split the dataset into training and validating sets (80/20)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "clf_nb_h = GaussianNB()\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf_nb_h.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the validating data\n",
    "y_predict = clf_nb_h.predict(x_val)\n",
    "\n",
    "# Evaluate model performance\n",
    "print('\\nHoldout result:')\n",
    "accuracy_score_holdout = accuracy_score(y_val, y_predict)\n",
    "print('\\nAccuracy  -->', accuracy_score_holdout)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a1184",
   "metadata": {},
   "source": [
    "<div id=\"rrs\"> \n",
    "    <h2>Repeated Random Sampling</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8e7a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated random sampling\n",
    "\n",
    "Accuracy = []             # Initialize a list to store accuracy results\n",
    "num_repeats = 10          # Number of times to repeat random sampling\n",
    "\n",
    "# Perform repeated random sampling\n",
    "for i in range(num_repeats):\n",
    "    \n",
    "    # Split the dataset into training and validating sets (80/20)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Train a Naive Bayes classifier\n",
    "    clf_nb_rrs = GaussianNB()\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    clf_nb_rrs.fit(x_train, y_train)\n",
    "\n",
    "    # Predict the labels for the validating data\n",
    "    y_predict = clf_nb_rrs.predict(x_val)\n",
    "    accuracy_score(y_val, y_predict)\n",
    "    Accuracy.append(accuracy_score(y_val, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ddb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance        \n",
    "df_Accuracy = pd.DataFrame(Accuracy, columns=['Accuracy'])\n",
    "print('\\nAccuracy in 10 iterations for different train and validation sets:\\n')\n",
    "display(df_Accuracy)\n",
    "accuracy_score_rrs = df_Accuracy.Accuracy.mean()\n",
    "print('\\nThe mean of different accuracies for validating the model -->', accuracy_score_rrs)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc9688",
   "metadata": {},
   "source": [
    "<div id=\"test\"> \n",
    "    <h2>Testing</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5b734195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Train a Naive Bayes classifier\n",
    "clf_nb = GaussianNB()\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf_nb.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "Y_predict = clf_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726af42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "print('\\nTesting the model:\\n')\n",
    "\n",
    "accuracy_score_nb_testing = accuracy_score(Y_test, Y_predict)\n",
    "print('\\nAccuracy  -->', accuracy_score_nb_testing)\n",
    "print('\\nRecall or Sensitivity or TPR --->', recall_score(Y_test, Y_predict))\n",
    "print('\\nPrecision -->', precision_score(Y_test, Y_predict))\n",
    "print('\\nF1_score -->', f1_score(Y_test, Y_predict))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the classification report\n",
    "print('\\nClassification report:\\n', classification_report(Y_test, Y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the confusion matrix\n",
    "confusion_matrix = metrics.confusion_matrix(Y_predict, Y_test)\n",
    "\n",
    "# Create a dataframe for the confusion matrix for better visualization\n",
    "confusion_matrix_dataframe = pd.DataFrame(confusion_matrix, columns = ['benign present', 'malignant present'], \n",
    "                                                            index = ['test benign', 'test malignant'])\n",
    "print(\"\\nConfusion matrix:\\n\")\n",
    "display(confusion_matrix_dataframe)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9c501",
   "metadata": {},
   "source": [
    "<div id=\"roc\"> \n",
    "    <h2>ROC plot and AUC score</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a76210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "def plot_roc_curve(y_test, y_prid):\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prid)      \n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC plot and AUC score\n",
    "plot_roc_curve(Y_test, Y_predict)\n",
    "\n",
    "roc_auc_score = roc_auc_score(Y_test, Y_predict)\n",
    "print('\\nAUC score:', roc_auc_score)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258cac47",
   "metadata": {},
   "source": [
    "<div id=\"output\"> \n",
    "    <h2>Output the results</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7927ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results of different validation methods and the Naive Bayes testing  \n",
    "print('\\nHoldout result:', accuracy_score_holdout)                       # Print accuracy score for the holdout method  \n",
    "print('\\nRepeated random sampling result:', accuracy_score_rrs)          # Print accuracy score for repeated random sampling method  \n",
    "print('\\nNaive Bayes testing result:', accuracy_score_nb_testing)        # Print accuracy score for naive bayes testing  \n",
    "print('\\nAUC score:', roc_auc_score)                                     # Print AUC score for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc6283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
